{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep eXtreme MultiLabel (DXML) Classification\n",
    "\n",
    "A classification task is defined as 'extreme' when the number of classes considered grows considerably (from a dozen to millions of classes). The 'Multi-label' problem is similar to a classification task, but each sample may have more than one associated label. Furthermore, any sample may have a different number of associated labels.\n",
    "\n",
    "This notebook aims to prove that naive approaches may not be neither performant nor mathematically correct for this problem, and to show a simplified version of the XML method developed in paper [**Deep Extreme Multi-label Learning**](https://arxiv.org/pdf/1704.03718.pdf) by Zhang et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory: label independance\n",
    "\n",
    "A multi-label problem is different from a multi-class one as the latter only attributes the most probable class for a sample, whereas the multi-label problem needs to attribute the most probable subset of classes.\n",
    "\n",
    "For instance, in visual recognition, a multi-class problem would be to recognise the main subject of a picture whereas a multi-label problem would be the recognition of all recognizable objects in the picture.\n",
    "\n",
    "At first one could think that the subset of the most probable classes would form the most probable subset of classes, but it is not true on the general case. It can only be true if each class probability is independent from the others (or $P(A\\,|\\, B) = P(A) $ for each A,B classes in our problem). **This hypothesis can't possibly be satisfied in most of our problems**.\n",
    "\n",
    "That's what makes mult-label so tricky. Indeed the number of possible subsets is much larger than the number of classes (growing as $2^N$, with N classes. That means multilabelling with 100 labels makes a little more than a 1 million classes classification problem. Scaling to 100 labels would have us have to choose between $ 1,26 . 10^{30}$ possible subsets.\n",
    "\n",
    "Thus there's a need for a reasonable approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birds chant recognition\n",
    "\n",
    "We'll follow through this reasoning using data from [The 9th annual MLSP competition: New methods for acoustic classification of multiple simultaneous bird species in a noisy environment]( http://vintage.winklerbros.net/Publications/mlsp2013ma.pdf ).\n",
    "\n",
    "This data represents audio recordings of birds chants, with the objective of recognising up to 19 different birds. It has been split for the sake of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322, 260)\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X_train, X_test = np.loadtxt('data/X_train_birds.txt'), np.loadtxt('data/X_test_birds.txt')\n",
    "y_train, y_test = np.loadtxt('data/y_train_birds.txt'), np.loadtxt('data/y_test_birds.txt')\n",
    "birds_list = [\"Brown Creeper\",\"Pacific Wren\",\"Pacific-slope Flycatcher\",\"Red-breasted Nuthatch\",\n",
    "              \"Dark-eyed Junco\",\"Olive-sided Flycatcher\",\"Hermit Thrush\",\"Chestnut-backed Chickadee\",\n",
    "              \"Varied Thrush\",\"Hermit Warbler\",\"Swainson's Thrush\",\"Hammond's Flycatcher\",\n",
    "              \"Western Tanager\",\"Black-headed Grosbeak\",\"Golden Crowned Kinglet\",\"Warbling Vireo\",\n",
    "              \"MacGillivray's Warbler\",\"Stellar's Jay\",\"Common Nighthawk\"]\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">Let's look at the data to picture the multi-label problem. Observe and comment on the data: check that this is a multi-label problem. What problems could arise if we applied a global \"blind\" method?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322  train samples and  323  test sample\n",
      "###\n",
      " Species Brown Creeper appears 14.0 times\n",
      " Species Pacific Wren appears 81.0 times\n",
      " Species Pacific-slope Flycatcher appears 46.0 times\n",
      " Species Red-breasted Nuthatch appears 9.0 times\n",
      " Species Dark-eyed Junco appears 20.0 times\n",
      " Species Olive-sided Flycatcher appears 14.0 times\n",
      " Species Hermit Thrush appears 47.0 times\n",
      " Species Chestnut-backed Chickadee appears 40.0 times\n",
      " Species Varied Thrush appears 61.0 times\n",
      " Species Hermit Warbler appears 53.0 times\n",
      " Species Swainson's Thrush appears 103.0 times\n",
      " Species Hammond's Flycatcher appears 28.0 times\n",
      " Species Western Tanager appears 33.0 times\n",
      " Species Black-headed Grosbeak appears 9.0 times\n",
      " Species Golden Crowned Kinglet appears 37.0 times\n",
      " Species Warbling Vireo appears 17.0 times\n",
      " Species MacGillivray's Warbler appears 6.0 times\n",
      " Species Stellar's Jay appears 10.0 times\n",
      " Species Common Nighthawk appears 26.0 times\n",
      "###\n",
      "As a mean 1.013953488372093 species appear on each recording (standard deviation 1.1739249864474566)\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), ' train samples and ', len(X_test), ' test sample')\n",
    "print('###')\n",
    "for i in range(len(birds_list)):\n",
    "    print(' Species {} appears {} times'.format(birds_list[i], np.sum(y_train[:,i]) + np.sum(y_test[:,i])))\n",
    "print('###')\n",
    "all_y = np.concatenate([y_train, y_test])\n",
    "all_sum_y = np.sum(all_y, axis=1)\n",
    "print('As a mean {} species appear on each recording (standard deviation {})'.format(np.mean(all_sum_y), np.std(all_sum_y)))\n",
    "\n",
    "# Remember that the repartion of occurence between species is quite unequal, which would tend to bias black-block approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">(Optional) Implement a Multi-Layer Perceptron (MLP) classifier to check the difficulty of the task.</div>\n",
    "\n",
    "## Mathematically false approach: parallel classifiers\n",
    "\n",
    "<div class=\"alert alert-warning\">The intuitive approach that supposes independance of different labels would be to implement parallel classifiers. Let's look at their performance.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTION 1 : SKLEARN\n",
    "mlpc = MLPClassifier(hidden_layer_sizes=(64,32,32), activation='relu')\n",
    "mlpc.fit(X_train, y_train)\n",
    "mlpc.score(X_test, y_test), mlpc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Reshape, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv1D, Conv2D\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTION 2 : KERAS\n",
    "\n",
    "m = Sequential()\n",
    "# Original --------------------------------------------------Start\n",
    "m.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "m.add(Dense(32, activation='relu'))\n",
    "m.add(Dense(len(birds_list), activation='tanh'))\n",
    "m.summary()\n",
    "m.compile(loss='categorical_crossentropy', metrics=['categorical_accuracy'], optimizer=Adam())\n",
    "# Original --------------------------------------------------End\n",
    "\n",
    "h = m.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, \n",
    "           verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 322, 260)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 322, 260)          1040      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 322, 260)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 322, 1024)         267264    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 322, 1024)         4096      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 322, 1024)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 322, 512)          524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 322, 512)          2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 322, 512)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 322, 512)          262656    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 322, 300)          153900    \n",
      "=================================================================\n",
      "Total params: 1,215,804\n",
      "Trainable params: 1,212,212\n",
      "Non-trainable params: 3,592\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected reshape_1_input to have 3 dimensions, but got array with shape (322, 260)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0884dd0e2292>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m               metrics=['categorical_accuracy'])\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1630\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1631\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    111\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected reshape_1_input to have 3 dimensions, but got array with shape (322, 260)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# DXML reproduce\n",
    "\n",
    "# Embedding layer is not required now as the birds data is in feature value format rather than feature index format:\n",
    "#model.add(Embedding(max_features, embedding_dim=260, weights=[X_train], input_length=maxlen, trainable=True))\n",
    "model = Sequential()\n",
    "model.add(Reshape((X_train.shape[0], X_train.shape[1]),\n",
    "                  input_shape=(X_train.shape[0], X_train.shape[1])))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(filters=1024,\n",
    "                 kernel_size=1,\n",
    "                 kernel_initializer='uniform',\n",
    "                 activation='relu')\n",
    "          )\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(filters=512,\n",
    "                 kernel_size=1,\n",
    "                 activation='relu')\n",
    "          )\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(filters=512,\n",
    "                 kernel_size=1,\n",
    "                 activation='relu')\n",
    "          )\n",
    "model.add(Dense(300,\n",
    "                activation='relu')\n",
    "          )\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy']\n",
    "              )\n",
    "\n",
    "dxml_fit = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                     epochs=10, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Keras extra modules\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout, Flatten, regularizers\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers import Conv2D, GlobalMaxPooling1D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import argparse\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "K.set_image_dim_ordering('th')\n",
    "print(K.image_data_format())\n",
    "\n",
    "## required for efficient GPU use\n",
    "import tensorflow as tf\n",
    "from keras.backend import tensorflow_backend\n",
    "config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "session = tf.Session(config=config)\n",
    "tensorflow_backend.set_session(session)\n",
    "## required for efficient GPU use\n",
    "\n",
    "import os\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "# define path to save model\n",
    "model_path = './fm_cnn_BN.h5'\n",
    "# prepare callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_acc', \n",
    "        patience=10,\n",
    "        mode='max',\n",
    "        verbose=1),\n",
    "    ModelCheckpoint(model_path,\n",
    "        monitor='val_acc', \n",
    "        save_best_only=True, \n",
    "        mode='max',\n",
    "        verbose=0)\n",
    "]\n",
    "\n",
    "\n",
    "# get data\n",
    "test  = pd.read_csv('./fashion-mnist_test.csv')\n",
    "train = pd.read_csv('./fashion-mnist_train.csv')\n",
    "print('train shape: {}'.format(train.shape))\n",
    "print('test shape: {}'.format(test.shape))\n",
    "\n",
    "#reshape data\n",
    "y_train_CNN = train.ix[:,0].values.astype('int32') # only labels i.e targets digits\n",
    "X_train_CNN = np.array(train.iloc[:,1:].values).reshape(train.shape[0], 1, 28, 28).astype(np.uint8)# reshape to be [samples][pixels][width][height]\n",
    "print('train shape after reshape: {}'.format(X_train_CNN.shape))\n",
    "\n",
    "y_test_CNN = test.ix[:,0].values.astype('int32') # only labels i.e targets digits\n",
    "X_test_CNN = np.array(test.iloc[:,1:].values).reshape((test.shape[0], 1, 28, 28)).astype(np.uint8)\n",
    "print('test shape after reshape: {}'.format(X_test_CNN.shape))\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train_CNN = to_categorical(y_train_CNN)\n",
    "y_test_CNN = to_categorical(y_test_CNN)\n",
    "num_classes = y_train_CNN.shape[1]\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train_CNN = X_train_CNN / 255\n",
    "X_test_CNN = X_test_CNN / 255\n",
    "\n",
    "#size of parameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "filter_pixel=3\n",
    "noise = 1\n",
    "droprate=0.25\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "input_shape = (1, img_rows, img_cols)\n",
    "\n",
    "#Start Neural Network\n",
    "model = Sequential()\n",
    "#convolution 1st layer\n",
    "model.add(Conv2D(64, kernel_size=(filter_pixel, filter_pixel), padding=\"same\",\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape)) #0\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(droprate))#3\n",
    "#model.add(MaxPooling2D())\n",
    "\n",
    "#convolution 2nd layer\n",
    "model.add(Conv2D(64, kernel_size=(filter_pixel, filter_pixel), activation='relu',border_mode=\"same\"))#1\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(droprate))#3\n",
    "\n",
    "#convolution 3rd layer\n",
    "model.add(Conv2D(64, kernel_size=(filter_pixel, filter_pixel), activation='relu',border_mode=\"same\"))#1\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(droprate))#3\n",
    "\n",
    "#Fully connected 1st layer\n",
    "model.add(Flatten()) #7\n",
    "model.add(Dense(500,use_bias=False)) #13\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu')) #14\n",
    "model.add(Dropout(droprate))      #15\n",
    "\n",
    "#Fully connected final layer\n",
    "model.add(Dense(10)) #8\n",
    "model.add(Activation('softmax')) #9\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Save Model=ON\n",
    "history = model.fit(X_train_CNN, y_train_CNN,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_CNN, y_test_CNN),shuffle=True,callbacks=callbacks)\n",
    "\n",
    "score = model.evaluate(X_test_CNN, y_test_CNN, verbose=0)\n",
    "\n",
    "#print loss and accuracy\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTION\n",
    "plt.figure()\n",
    "plt.plot(h.history['categorical_accuracy'], c='blue', label='train')\n",
    "plt.plot(h.history['val_categorical_accuracy'], c='red', label='test')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Accuracy of the MLP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitely, MLPs are not very convincing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: parallel classifiers\n",
    "\n",
    "<div class=\"alert alert-warning\">The intuitive approach (which you now know is inexact!): Individually predict each label (for example, use Naive Bayes classifiers).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CORRECTION\n",
    "classif_list = []\n",
    "train_score_list, test_score_list = [], []\n",
    "\n",
    "# Training\n",
    "for i in range(len(birds_list)):\n",
    "    y_train_for_this_bird = y_train[:,i]\n",
    "    new_classifier = GaussianNB()\n",
    "    new_classifier.fit(X_train, y_train_for_this_bird)\n",
    "    classif_list.append(new_classifier)\n",
    "\n",
    "# Test & display results\n",
    "for i in range(len(birds_list)):\n",
    "    classif = classif_list[i]\n",
    "    train_score = float('{0:.3f}'.format(classif.score(X_train, y_train[:,i])))\n",
    "    test_score = float('{0:.3f}'.format(classif.score(X_test, y_test[:,i])))\n",
    "    train_score_list.append(train_score)\n",
    "    test_score_list.append(test_score)\n",
    "    print('Detecting {} with {}% accuracy (training {}%)'.format(birds_list[i], 100*test_score, 100*train_score))\n",
    "\n",
    "predict_train = np.zeros_like(y_train)\n",
    "predict_test = np.zeros_like(y_test)\n",
    "for i in range(len(birds_list)):\n",
    "    classif = classif_list[i]\n",
    "    predict_train[:,i] = classif.predict(X_train)\n",
    "    predict_test[:,i] = classif.predict(X_test)\n",
    "acc_train = 1 - np.sum(np.abs(predict_train - y_train))/(y_train.shape[0]*y_train.shape[1])\n",
    "acc_test = 1 - np.sum(np.abs(predict_test - y_test))/(y_test.shape[0]*y_test.shape[1])\n",
    "print('###')\n",
    "print('Global accuracy: testing {}, training {}'.format(acc_test, acc_train))\n",
    "\n",
    "well_labeled = 0\n",
    "for i in range(len(y_train)):\n",
    "    if np.sum(np.abs(y_train[i,:] - predict_train[i,:])) == 0:\n",
    "        well_labeled +=1\n",
    "print('Overall {} out of the {} training samples were well labeled'.format(well_labeled,len(y_train)))\n",
    "\n",
    "well_labeled = 0\n",
    "for i in range(len(y_test)):\n",
    "    if np.sum(np.abs(y_test[i,:] - predict_test[i,:])) == 0:\n",
    "        well_labeled +=1\n",
    "print('Overall {} out of the {} testing samples were well labeled'.format(well_labeled,len(y_test)))\n",
    "\n",
    "# Of course, in addition to being mathematically frowned upon, this method is cumbersome,\n",
    "# imagine having to build a million classifiers! It is not at all extensible to extreme classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">Remember that those results derive from an inadequate method. The Multi - Label classification and its XL version, the eXtreme MultiLabel Classification (XML) are difficult to treat problems on which the classical methods can not be used as they are.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding 101\n",
    "\n",
    "Embedding is a projection of data in another space. As we can specifically design this destination space, we'll be able to highlight specific relation between data points that may have been hidden before.\n",
    "\n",
    "For instance:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Dog</td>\n",
    "        <td>Cat</td>\n",
    "        <td>Elephant</td>\n",
    "        <td>Turtle</td>\n",
    "        <td>Fox</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Dog</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Cat</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Fox</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "could be projected as:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Dimension 1</td>\n",
    "        <td>Dimension 2</td>\n",
    "        <td>Dimension 3</td>\n",
    "        <td>Dimension 4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Dog</td>\n",
    "        <td>0.123</td>\n",
    "        <td>0.452</td>\n",
    "        <td>0.865</td>\n",
    "        <td>0.852</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Cat</td>\n",
    "        <td>0.458</td>\n",
    "        <td>0.741</td>\n",
    "        <td>0.852</td>\n",
    "        <td>0.963</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Fox</td>\n",
    "        <td>0.789</td>\n",
    "        <td>0.456</td>\n",
    "        <td>0.845</td>\n",
    "        <td>0.654</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Embedding is used a lot in Natural Language Processing to create mathematically just syntaxical or lexical relations, but in this problem, it will enable us to relate one labels to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Zhang et. al. method : Interaction between labels Embedding\n",
    "\n",
    "Their method can be decomposed in 4 very simple steps, as we'll dispense from the 'deep' part thanks to our relatively small problem.\n",
    "\n",
    "### First step: compute labels adjacency\n",
    "\n",
    "Let's look at our labels to see how close they are to each other. We'll draw the adjacency matrix and adjacency graph. In the latter, each node is a label and a vertex symbolizes the number of conjoint appearances of two labels.\n",
    "\n",
    "### Second step: learning adjacency-based label embedding\n",
    "\n",
    "The cited paper uses DeepWalk to form an embedding based on the adjacency graph.\n",
    "\n",
    "In our example here we'll do with a linear projection using the adjacency matrix, which despite being much more simple will prove its value.\n",
    "\n",
    "### Third step : prediction in embedded space\n",
    "\n",
    "Instead of doing a classification problem from the features to the labels, we'll train a multidimensional regression agent from the features to the embedded space, which will yield coordinates for a data sample in this continuous space.\n",
    "\n",
    "### Fourth step : return to labels\n",
    "\n",
    "From our embedded space coordinates, we'll run a k-Nearest Neighbors (kNN) to find the closest labels and return them as a prediction.\n",
    "\n",
    "\n",
    "Here's the original, full-scale full-deep method as proposed in the original article:\n",
    "<img src=\"./images/labelmatrix.png\" alt=\"Article-recap\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## On our example\n",
    "\n",
    "### Compute adjacency matrix\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\"> Let's build the labels proximity matrix as $M_{[i,j] \\in [|1,19|]^2}$ so that $m_{i,j}$ is the joint apparition frequency of labels $i$ and $j$.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trick\n",
    "#Visualizing a 19 * 19 matrix is not necessarily obvious.\n",
    "#You will find below a simple method to represent a matrix as an image.\n",
    "\n",
    "my_matrix = np.random.normal(loc=1, scale=0.3, size=(25,25)) # matrix to represent\n",
    "plt.figure()\n",
    "plt.imshow(my_matrix,cmap=\"hot\")\n",
    "plt.colorbar()\n",
    "plt.title('Visualization of the matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll rather visualize an image than a 19*19 matrix.\n",
    "\n",
    "matrix = np.zeros((y_train.shape[1],y_train.shape[1]))\n",
    "edges =[]\n",
    "for row in y_train : # We take the training points one by one\n",
    "    for i in range(len(row)):\n",
    "        for j in range(len(row)):            \n",
    "            if row[i]*row[j] == 1: # both labels are activated \n",
    "                matrix[i,j] += 1\n",
    "                if ([i,j] not in edges) and ([j,i] not in edges) and not i==j:\n",
    "                    edges.append([i,j])\n",
    "\n",
    "#to get frequency we'll divide by number of appearances\n",
    "norm_matrix = np.copy(matrix)\n",
    "#np.save(norm_matrix,'norm_matrix.npm')\n",
    "for i in range(matrix.shape[0]):\n",
    "    d = norm_matrix[i,i]\n",
    "    norm_matrix[:,i] = norm_matrix[:,i]/d\n",
    "    norm_matrix[i,:] = norm_matrix[i,:]/d\n",
    "    norm_matrix[i,i] = 1\n",
    "\n",
    "# Visualize\n",
    "plt.figure()\n",
    "plt.imshow(norm_matrix,cmap=\"hot\")\n",
    "plt.colorbar()\n",
    "plt.title('Label proximity (absolute)')\n",
    "plt.show()\n",
    "\n",
    "# Diagonal coefficients are not interesting, let's ditch them\n",
    "norm_matrix -= np.identity(norm_matrix.shape[0])\n",
    "plt.figure()\n",
    "plt.imshow(norm_matrix,cmap=\"hot\")\n",
    "plt.colorbar()\n",
    "plt.title('Label proximity (without diagonal)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacency graph\n",
    "\n",
    "This graph is actually used in Zhang et al's method, but for us it will only be a data visualization tool.\n",
    "\n",
    "Here's the **draw_graph** function that could be used in other projects too.\n",
    "\n",
    "Another way of representing the proximity between labels is to construct a weighted graph. This is necessary for Zhang et al, who use this graph for their embedding, but ours will only require the adjacency matrix. We recommend that you build this graph at a minimum in order to visualize the data.\n",
    "\n",
    "We put at your disposal the function draw_graph which will automatically build it from a list of connected labels and the proximity matrix you just calculated. The threshold option allows you to choose from how many occurrences a link will be represented on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx #For those among you that use Anaconda dist, you already got this\n",
    "\n",
    "def draw_graph(edges, weights_matrix=None, threshold=0, figsize=(8,8)):\n",
    "    \n",
    "    '''\n",
    "    edges : lists of connected labels\n",
    "    weights_matrix : labels proximity matrix\n",
    "    threshold : number of occurence required to draw a connection\n",
    "    figsize : size of the plot to display\n",
    "    '''\n",
    "    \n",
    "    edges = [edge for edge in edges if weights_matrix[edge[0],edge[1]] > threshold]\n",
    "    \n",
    "    # additional settings (you can mess around here)\n",
    "    node_size = 1600\n",
    "    node_color = 'blue'\n",
    "    node_alpha = 0.2\n",
    "    node_text_size = 12\n",
    "    edge_color = 'blue'\n",
    "    edge_alpha= 0.3\n",
    "    edge_tickness = 1\n",
    "    edge_text_pos = 0.3\n",
    "    text_font = 'sans-serif'\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    # create networkx graph\n",
    "    G=nx.Graph()\n",
    "\n",
    "    # add edges\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[0], edge[1])\n",
    "\n",
    "    # select shell autolocation\n",
    "    graph_pos=nx.shell_layout(G)\n",
    "\n",
    "    # draw graph\n",
    "    nx.draw_networkx_nodes(G,\n",
    "                           graph_pos,\n",
    "                           node_size=node_size, \n",
    "                           alpha=node_alpha,\n",
    "                           node_color=node_color)\n",
    "    nx.draw_networkx_edges(G, graph_pos,width=edge_tickness, alpha=edge_alpha,edge_color=edge_color)\n",
    "    nx.draw_networkx_labels(G, graph_pos,font_size=node_text_size, font_family=text_font)\n",
    "    \n",
    "    # construct weights dict\n",
    "    weights={}\n",
    "    for i in range(len(edges)):\n",
    "        weights[tuple(edges[i])] = weights_matrix.astype(int)[edges[i][0],edges[i][1]]\n",
    "    \n",
    "    # draw weights\n",
    "    edge_labels = weights\n",
    "    nx.draw_networkx_edge_labels(G, graph_pos, edge_labels=edge_labels, \n",
    "                                 label_pos=edge_text_pos)\n",
    "\n",
    "    # show graph\n",
    "    plt.title('Proximity graph between labels with a proximity threshold at {} occurrences'.format(threshold))\n",
    "    plt.show()\n",
    "    \n",
    "# Exemple\n",
    "graph = [(0, 1), (1, 4), (2, 3), (0, 4), (1, 3)]\n",
    "weights_matrix = np.random.randint(1,high=5,size=(5,5))\n",
    "draw_graph(graph,weights_matrix)\n",
    "draw_graph(graph,weights_matrix, threshold=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> Let's look at some graphs</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CORRECTION\n",
    "\n",
    "draw_graph(edges, matrix, figsize=(13,13), threshold=0)\n",
    "draw_graph(edges, matrix, figsize=(8,8), threshold=5)\n",
    "draw_graph(edges, matrix, figsize=(8,8), threshold=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the embedding\n",
    "\n",
    "Zhang et al use [**Deepwalk**](https://github.com/phanein/deepwalk) on the adjacency graph to build their embeding.\n",
    "\n",
    "Once the graph is constructed, Zhang et al use Deepwalk to reduce the size of their labels, that is, project them into a continuous space of smaller size.\n",
    "\n",
    "At our scale, we can content ourselves with making a simple multiplication of our labels vectors by the standardized proximity matrix that we have constructed previously, which will project our labels in a continuous space of dimension 19. It is in this space Continuing that we are going to do a regression learning.\n",
    "\n",
    "In our problem, multiplying the label vectors by the normalized adjacency matrix will project our labels into a continuous space of 19 dimensions. Our regression will be conducted in this space.\n",
    "\n",
    "For the moment, we are going to set up the Embedding in this continuous space. Construct a ** embedding_encode ** function that multiplies one or more samples by the matrix to the continuous space (adjacency matrix) and then centers and collapses; apply it to y_train and y_test.\n",
    "\n",
    "For example, we want to create a function such that: f ([0,0,0,0,0,0,1,0,0,1]) = [0.25, 0.32, 0.87]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_encode(y, matrix, scalor=None, return_scalor=False):\n",
    "    y_conti = np.dot(y, norm_matrix)\n",
    "    if scalor:\n",
    "        y_conti=scalor.transform(y_conti)\n",
    "    else:\n",
    "        scalor = StandardScaler()\n",
    "        y_conti = scalor.fit_transform(y_conti)\n",
    "    \n",
    "    if return_scalor:\n",
    "        return y_conti, scalor\n",
    "    else:\n",
    "        return y_conti\n",
    "\n",
    "y_conti_train, scalor = embedding_encode(y_train, norm_matrix, return_scalor=True)\n",
    "y_conti_test = embedding_encode(y_test, norm_matrix, scalor=scalor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoding part will be performed by a kNN research in the continuous space.\n",
    "\n",
    "<div class=\"alert alert-warning\"> The * KNearestNeighborsClassifier * class you can import from * sklearn.neighbors * has a * k neighbors * method that returns the nearest n $ $ indices. Use it to create a ** encoding_decode ** method that lets you move from a sample of continuous space to the label space.</div> \n",
    "\n",
    "(If you prefer to implement directly or otherwise the KNN algorithm, it's to your taste!)\n",
    "\n",
    "For example, we want to create a function such that: f ([0.25, 0.32, 0.87]) = [0,0,0,0,0,0,1,1,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(y_conti_train, y_train)\n",
    "\n",
    "def embedding_decode(y, knn, neighbors_number=2, algorithm='brute', y_train=y_train):\n",
    "    indexes_of_neighbors = knn.kneighbors(y,n_neighbors=neighbors_number, return_distance=False)\n",
    "    number_of_samples = indexes_of_neighbors.shape[0]\n",
    "    labels = np.zeros((number_of_samples, y_train.shape[1]))\n",
    "    \n",
    "    for i in range(number_of_samples):\n",
    "        for j in range(neighbors_number):\n",
    "            labels[i, :] += y_train[indexes_of_neighbors[i,j],:]\n",
    "        for j in range(labels.shape[1]):\n",
    "            if labels[i,j]: labels[i,j] = 1\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# small optimization on neighbors_number\n",
    "err_list=[]\n",
    "err_list_t=[]\n",
    "for n in range(2,30):\n",
    "    y_train_decoded = embedding_decode(y_conti_train, knn, n )\n",
    "    y_test_decoded = embedding_decode(y_conti_test, knn , n)\n",
    "    error = np.sum(np.abs(y_train_decoded - y_train))\n",
    "    error_t = np.sum(np.abs(y_test_decoded - y_test))\n",
    "    err_list.append(error/(y_train.shape[0]*y_train.shape[1]))\n",
    "    err_list_t.append(error_t/(y_test.shape[0]*y_test.shape[1]))\n",
    "plt.figure()\n",
    "plt.plot(err_list, label='training data')\n",
    "plt.plot(err_list_t, label='testing data', color='red')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Error from number of selected neighbours')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional Regression\n",
    "\n",
    "We now have a way to move from the space of the labels to their continuous representation and vice versa, it remains for us to implement the regression in the continuous space to complete the method!\n",
    "\n",
    "<div class=\"alert alert-warning\"> Create a regressor (for example, use RandomForestRegressor) predicting continuous values (* y_conti *) from features (* X_train, X_test *), and then use the ** embedding_decode ** function previously coded to return to the label space. Compare the accuracy obtained with that of your parallel classifiers. </div>\n",
    "\n",
    "We'll implement a simple model to predict coordinates in our embedded space from features, such as a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# mini-optimization\n",
    "# This estimator is the one who realizes the true \"prediction\", a good precision is more than desirable\n",
    "# We thus realize a mini-optimization on the number of arbs of our Random Forest\n",
    "list_train, list_test = [],[]\n",
    "for n in range(100,1101,200):\n",
    "    regressor = RandomForestRegressor(n_estimators=n, n_jobs=-1)\n",
    "    regressor.fit(X_train, y_conti_train)\n",
    "    list_train.append(regressor.score(X_train,y_conti_train))\n",
    "    list_test.append(regressor.score(X_test,y_conti_test))\n",
    "plt.figure()\n",
    "plt.plot(list_train, label='score training')\n",
    "plt.plot(list_test, label='score testing', color='red')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual regression\n",
    "regressor = RandomForestRegressor(n_estimators = 400, n_jobs=-1)\n",
    "regressor.fit(X_train, y_conti_train)\n",
    "y_conti_predict = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion in labels\n",
    "y_predict = embedding_decode(y_conti_predict, knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the full method precision\n",
    "well_labeled = 0\n",
    "for i in range(len(y_test)):\n",
    "    if np.array_equal(y_test[i,:] , y_predict[i,:]):\n",
    "        well_labeled +=1\n",
    "print('Overall {} out of the {} testing samples were well labeled'.format(well_labeled,len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> Although our embedding is reduced to its simplest expression and our regressor is a questionable prediction, taking into account the adjacency of the labels has made it possible to pass from 24 to 141 well-ranked samples on 323 compared to our baseline of 19 parallel naive-bayes classifiers. The added value of the method is undeniable both mathematically and practically!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main limitation of the method is related to the function passing from the embedding space to the space of the labels. Indeed, the k-Nearest-Neighbors function takes as attribute the number of neighbors to be selected. This number is therefore fixed for all the samples. This implies that the same number of labels will be predicted for all samples.\n",
    "\n",
    "# For further details:\n",
    "\n",
    "Here are more XML datasets to have fun with:\n",
    "    - http://mulan.sourceforge.net/datasets-mlc.html \n",
    "    - http://manikvarma.org/downloads/XC/XMLRepository.html\n",
    "\n",
    "You can try on these to build the adjacency graph and use [**Deepwalk**](https://github.com/phanein/deepwalk) on it !\n",
    "\n",
    "Deepwalk is an algorithm that uses random walks to learn a representation of the nodes of a graph. Deepwalk uses the information obtained after random walks by considering the sequence of nodes encountered. This method has an advantage, it is adaptable. It is not necessary to restart the process from the beginning to add new nodes and relationships."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_metadata": {
   "author": "Timon Ther",
   "title": "eXtreme MultiLabel Classification"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2.0,
    "version_minor": 0.0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
